import os
import openai
from dotenv import load_dotenv, find_dotenv

from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.text_splitter import TokenTextSplitter
from langchain.text_splitter import MarkdownHeaderTextSplitter

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import NotionDirectoryLoader

# ğŸ” Carrega chave da OpenAI do .env
_ = load_dotenv(find_dotenv()) 
openai.api_key  = os.environ['OPENAI_API_KEY']

# CharacterTextSplitter
# Faz cortes brutos com base em um Ãºnico separador e tamanho fixo (ex: espaÃ§o ou \n)
# NÃ£o considera contexto semÃ¢ntico â†’ pode cortar no meio de uma frase ou palavra

# RecursiveCharacterTextSplitter
# Tenta cortar de forma "inteligente", seguindo uma hierarquia de separadores (parÃ¡grafos > linhas > frases > palavras > caractere bruto)
# Preserva melhor o contexto entre chunks, ideal para textos longos e estruturados

# TokenTextSplitter
# Separa os chunks por nÃºmero de tokens. Pode ser Ãºtil ao considerar que o contexto colocado no prompt tem limites baseados em nÃºmero de tokens.

# MarkdownHeaderTextSplitter
# Separa os chunks por headers no markdown e armazena seus metadados

def split_raw_string():
    chunk_size = 26
    chunk_overlap = 4

    r_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    c_splitter = CharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )

    # ğŸ”¤ Texto contÃ­nuo sem espaÃ§os (apenas letras do alfabeto)
    text1 = 'abcdefghijklmnopqrstuvwxyz'
    print("\nğŸ” Recursive split (text1):")
    print(r_splitter.split_text(text1))

    text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'
    print("\nğŸ” Recursive split (text2):")
    print(r_splitter.split_text(text2))

    # ğŸ§© Texto com espaÃ§os entre letras
    text3 = "a b c d e f g h i j k l m n o p q r s t u v w x y z"

    print("\nğŸ” Recursive split (text3 - com espaÃ§os):")
    print(r_splitter.split_text(text3))

    print("\nğŸ” Character split (text3 - com espaÃ§os):")
    print(c_splitter.split_text(text3))

    # ğŸ”§ ForÃ§a o separador do CharacterTextSplitter como ' '
    c_splitter_custom = CharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separator=' '
    )
    print("\nğŸ” Character split (com separador explÃ­cito ' '):")
    print(c_splitter_custom.split_text(text3))

def split_document_text():
    # ğŸ“„ Texto estruturado com parÃ¡grafos, frases e espaÃ§os
    some_text = """Ao escrever documentos, os autores utilizam a estrutura do documento para agrupar o conteÃºdo.
    Isso pode transmitir ao leitor quais ideias estÃ£o relacionadas. Por exemplo, ideias intimamente relacionadas estÃ£o em sentenÃ§as. Ideias semelhantes estÃ£o em parÃ¡grafos. Os parÃ¡grafos formam um documento.

    ParÃ¡grafos geralmente sÃ£o delimitados por uma quebra de linha ou duas quebras de linha.
    As quebras de linha sÃ£o o "barra n" (\\n) que vocÃª vÃª incorporado nessa string.
    As sentenÃ§as terminam com um ponto final, mas tambÃ©m tÃªm um espaÃ§o.
    E as palavras sÃ£o separadas por espaÃ§os."""

    print(f"\nğŸ“ Tamanho do texto original: {len(some_text)} caracteres\n")

    # ğŸ”¹ CharacterTextSplitter com separador por espaÃ§o
    c_splitter = CharacterTextSplitter(
        chunk_size=450,
        chunk_overlap=0,
        separator=' '
    )

    # ğŸ”· RecursiveCharacterTextSplitter com separadores customizados
    r_splitter = RecursiveCharacterTextSplitter(
        chunk_size=450,
        chunk_overlap=0,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    # ğŸ”· RecursiveCharacterTextSplitter com separadores padrÃ£o
    r_default_splitter = RecursiveCharacterTextSplitter(
        chunk_size=450,
        chunk_overlap=0
    )

    print("ğŸ”¹ CharacterTextSplitter (por espaÃ§o):")
    print(*c_splitter.split_text(some_text), sep="\n")

    print("\nğŸ”· RecursiveCharacterTextSplitter (com separadores explÃ­citos):")
    print(*r_splitter.split_text(some_text), sep="\n")

    print("\nğŸ”· RecursiveCharacterTextSplitter (com separadores padrÃ£o):")
    print(*r_default_splitter.split_text(some_text), sep="\n")


    # Reduzindo um chunk e incluindo o ponto como separador

    print("\nğŸ”· RecursiveCharacterTextSplitter (\\n\\n, \\n, ., ' ', ''):")
    r_splitter1 = RecursiveCharacterTextSplitter(
        chunk_size=150,
        chunk_overlap=0,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    print(*r_splitter1.split_text(some_text), sep="\n")

    print("\nğŸ”· RecursiveCharacterTextSplitter (\\n\\n, \\n, (?<=\\. ), ' ', ''):")
    r_splitter2 = RecursiveCharacterTextSplitter(
        chunk_size=150,
        chunk_overlap=0,
        separators=["\n\n", "\n", "(?<=\. )", " ", ""]
    )

    print(*r_splitter2.split_text(some_text), sep="\n")

def split_pdf():
    loader = PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture01.pdf")
    pages = loader.load()
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=150,
        length_function=len
    )
    docs = text_splitter.split_documents(pages)
    print(len(docs))
    print(len(pages))

def split_notion():
    loader = NotionDirectoryLoader("docs/Notion_DB")
    notion_db = loader.load()

    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=150,
        length_function=len
    )
    docs = text_splitter.split_documents(notion_db)
    print(len(notion_db))
    print(len(docs))

def split_token():

    loader = PyPDFLoader("docs/cs229_lectures/MachineLearning-Lecture01.pdf")
    pages = loader.load()

    text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)
    text1 = "foo bar bazzyfoo"
    print(*text_splitter.split_text(text1), sep="\n")

    text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)
    docs = text_splitter.split_documents(pages)
    print(docs[0])
    print(pages[0].metadata)

def split_markdown():

    markdown_document = """# Title\n\n \
    ## Chapter 1\n\n \
    Hi this is Jim\n\n Hi this is Joe\n\n \
    ### Section \n\n \
    Hi this is Lance \n\n 
    ## Chapter 2\n\n \
    Hi this is Molly"""

    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]

    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on
    )
    md_header_splits = markdown_splitter.split_text(markdown_document)

    print(md_header_splits[0])
    print(md_header_splits[1])


    loader = NotionDirectoryLoader("docs/Notion_DB")
    docs = loader.load()
    txt = ' '.join([d.page_content for d in docs])

    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
    ]
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on
    )

    md_header_splits = markdown_splitter.split_text(txt)

    print (*md_header_splits[0], sep="\n")



# split_raw_string()
# split_document_text()
# split_pdf()
# split_notion()
# split_token()
split_markdown()