# ğŸ§ª LaboratÃ³rio LangChain com seus prÃ³prios dados

Este repositÃ³rio reÃºne anotaÃ§Ãµes e cÃ³digos experimentais baseados no curso ["LangChain: Chat with Your Data"](https://learn.deeplearning.ai/courses/langchain-chat-with-your-data/lesson/snupv/introduction), com foco em compreender e testar as etapas principais de processamento de documentos: **LOAD â†’ SPLIT â†’ EMBED â†’ RETRIEVE**.


## Baseado no curso em:

https://learn.deeplearning.ai/courses/langchain-chat-with-your-data/lesson/snupv/introduction


## ğŸ“„ Documento usado nos testes

O PDF referenciado em alguns exemplos de cÃ³digo vem do curso de Stanford. Para baixÃ¡-lo, execute:

```bash
curl -O https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf
```

Salve em: **./docs/cs229_lectures/**

## RecomendÃ¡vel criar o ambiente com virtualenv

```
virtualenv env
source env/bin/activate
```

## DependÃªncias

```
pip install langchain pypdf yt_dlp pydub dotenv openai langchain-community beautifulsoup4 tiktoken langchain-openai langchain_chroma chromadb lark scikit-learn
```

## O processo com o LangChain ocorre nas etapas LOAD, SPLIT, EMBED, RETRIEVE

### 1 - LOAD

Uma parte importante do LangChain sÃ£o seus DocumentLoaders, que podem carregar:

**Dados pÃºblicos**
- dados nÃ£o estruturados: youtube, wikipedia, twiter etc.
- dados estruturados: datasets, apis, csv etc.

**Dados privados**
- dados nÃ£o estruturados: power-point, notion, whatsapp etc.
- dados estruturados: pandas, excel, stripe, elastic etc.


#### 2 - SPLIT
ApÃ³s o carregamento, os documentos precisam ser divididos em pedaÃ§os menores (chunks) para que possam ser processados e indexados eficientemente â€” especialmente em sistemas de busca vetorial.

##### Desafios do split
Dividir um texto apenas por nÃºmero de caracteres pode cortar frases ou ideias importantes no meio, dificultando a recuperaÃ§Ã£o de informaÃ§Ãµes relevantes.

##### EstratÃ©gias para preservar contexto
Uma prÃ¡tica comum Ã© usar overlapping entre chunks:

Por exemplo, se o chunk A vai de 0 a 500 tokens, o chunk B pode comeÃ§ar no token 400 e ir atÃ© o 900, criando uma sobreposiÃ§Ã£o de 100 tokens. Isso ajuda a manter o contexto fluido entre trechos consecutivos.


##### Principais Tipos de Text Splitters no LangChain

Os Text Splitters sÃ£o responsÃ¡veis por dividir documentos em pedaÃ§os menores (**chunks**) para facilitar o processamento, indexaÃ§Ã£o e busca. A escolha do splitter certo pode impactar diretamente a qualidade das respostas em sistemas baseados em RAG (retrieval-augmented generation).

###### CharacterTextSplitter

Divide o texto com base em um nÃºmero fixo de caracteres, sem considerar a estrutura semÃ¢ntica. Ã‰ simples e rÃ¡pido, mas pode cortar ideias no meio se nÃ£o for bem parametrizado.

```python
from langchain.text_splitter import CharacterTextSplitter
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)
```

###### MarkdownHeaderTextSplitter
Divide documentos com base na hierarquia de cabeÃ§alhos Markdown (#, ##, ### etc.), preservando a estrutura do conteÃºdo. Ideal para documentaÃ§Ã£o tÃ©cnica.

```
from langchain.text_splitter import MarkdownHeaderTextSplitter
```

###### TokenTextSplitter
Usa contagem de tokens em vez de caracteres. Ã‰ Ãºtil para evitar que o chunk ultrapasse o limite de tokens de modelos como GPT.

```
from langchain.text_splitter import TokenTextSplitter
```

###### SentenceTransformersTokenTextSplitter
Usa embeddings para identificar pontos de corte semÃ¢ntico entre sentenÃ§as, mantendo a coesÃ£o do texto. Requer a instalaÃ§Ã£o do sentence-transformers.

```
from langchain.text_splitter import SentenceTransformersTokenTextSplitter
```

###### RecursiveCharacterTextSplitter
O splitter mais recomendado na maioria dos casos. Tenta dividir primeiro por seÃ§Ãµes maiores (como parÃ¡grafos, frases) e vai recursivamente atÃ© atingir o tamanho ideal. Preserva bem o contexto.

```
from langchain.text_splitter import RecursiveCharacterTextSplitter
```

###### NLTKTextSplitter
Baseado na biblioteca NLTK, divide textos em sentenÃ§as respeitando a pontuaÃ§Ã£o e a gramÃ¡tica. Requer o nltk instalado e seus recursos baixados.

```
from langchain.text_splitter import NLTKTextSplitter
```

###### SpacyTextSplitter
Semelhante ao NLTKTextSplitter, mas usa o spaCy para anÃ¡lise sintÃ¡tica mais precisa. Pode reconhecer entidades, frases nominais, etc.

```
from langchain.text_splitter import SpacyTextSplitter
```

###### Language (Code Splitter)

Pensado para cÃ³digo-fonte. Usa a linguagem de programaÃ§Ã£o como base para dividir trechos de cÃ³digo por classes, funÃ§Ãµes ou blocos. Muito Ãºtil em agentes que lidam com bases de cÃ³digo.

```
from langchain.text_splitter import Language
```


##### InformaÃ§Ãµes interessantes
- Os chunks armazenam metadados relacionados a sua fonte - um chunk separado a partir de um pdf pode ter o nome do arquivo e a pÃ¡gina em que foi produzido; um markdown pode ter a estrutura de cabeÃ§alhos na qual se originou; etc


### 3 - EMBED

ApÃ³s carregar o documento e dividi-lo em pedaÃ§os menores (chunks), Ã© necessÃ¡rio transformar cada chunk em um vetor numÃ©rico que represente seu conteÃºdo de forma semÃ¢ntica. Esse processo Ã© chamado de embedding, e o vetor resultante Ã© conhecido como um embedding vector (ou simplesmente embedding).

Esses vetores geralmente sÃ£o armazenados em uma base vetorial (vector store), permitindo buscas por similaridade com base no conteÃºdo, em vez de apenas palavras-chave.

Neste exemplo usamos o base vetorial Chroma, mas poderia ser o FAISS, Weaviate etc.

Importante notar que neste caso estamos usando o OpenAIEmbeddings, consumindo a API da OpenAI. A vetorizaÃ§Ã£o dos chunks **gera custo financeiro**. Em relaÃ§Ã£o a isso, algumas dicas:

### ğŸ“Š PreÃ§os (julho/2025)

- `text-embedding-3-small`: **0.02 USD por 1 milhÃ£o de tokens**
- `text-embedding-3-large`: **0.13 USD por 1 milhÃ£o de tokens**

Fonte oficial: [https://openai.com/pricing](https://openai.com/pricing)

### ğŸ”¢ Exemplo prÃ¡tico

Se vocÃª tiver 100 documentos com 500 tokens cada:

- Total de tokens: 100 Ã— 500 = **50.000 tokens**
- Custo estimado com `text-embedding-3-small`:  
  â†’ 50.000 Ã— 0.00002 USD = **0.01 USD**

### ğŸ’¡ Dicas para economizar

- Prefira o modelo `text-embedding-3-small`, que Ã© mais barato e eficiente.
- Ajuste o tamanho dos chunks para otimizar o nÃºmero de tokens por embedding.
- Use modelos **locais e gratuitos** caso a mÃ¡xima precisÃ£o da OpenAI nÃ£o seja necessÃ¡ria.

### ğŸ†“ Alternativas gratuitas (locais)

VocÃª pode usar modelos open-source com `HuggingFaceEmbeddings` ou `LangChain`:

- `all-MiniLM-L6-v2`
- `intfloat/e5-small-v2`
- `thenlper/gte-small`

Esses modelos funcionam bem em pipelines RAG, sem custo por token.


### 4 - RETRIEVAL

Nesta etapa, consultamos de fato a base semÃ¢ntica (vector store). A busca retorna, por padrÃ£o, os **chunks mais similares** Ã  consulta â€” ou seja, aqueles cujo *embedding* tem menor distÃ¢ncia vetorial em relaÃ§Ã£o ao *embedding* da pergunta.

No entanto, **os resultados mais similares nem sempre sÃ£o os mais Ãºteis**. Em muitos casos, pode ser mais interessante obter respostas que tragam **diversidade informativa**, e nÃ£o apenas repetiÃ§Ãµes do mesmo contexto. Ã‰ aqui que entra o algoritmo **MMR (Maximal Marginal Relevance)**.

O MMR busca um **equilÃ­brio entre relevÃ¢ncia e novidade**: resultados que sejam suficientemente **relacionados Ã  consulta**, mas ao mesmo tempo **diferentes entre si**, evitando redundÃ¢ncia.

#### ğŸ“Œ Exemplo ilustrativo

Imagine que um cozinheiro faz a pergunta:  
**"Quais cogumelos sÃ£o totalmente brancos?"**

- Os resultados mais similares podem descrever **uma Ãºnica espÃ©cie em detalhes** (como o champignon).
- PorÃ©m, um trecho que mencione que **"uma espÃ©cie branca Ã© venenosa"**, mesmo sendo menos similar, pode ser **crucial** â€” e o MMR ajuda a trazÃª-lo para os resultados.

â¡ï¸ Em resumo: **MMR busca relevÃ¢ncia com a consulta, mas diversidade em relaÃ§Ã£o aos demais resultados**.

---

### ğŸ” EstratÃ©gias adicionais de recuperaÃ§Ã£o

AlÃ©m da busca por similaridade ou MMR, existem outras estratÃ©gias que podem ser aplicadas em sistemas baseados em embeddings:

#### ğŸ“˜ LLM-Aided Retrieval

Algumas consultas possuem tanto um **elemento semÃ¢ntico** quanto um **filtro explÃ­cito**.  
Exemplo:  
**"Quais filmes de terror foram lanÃ§ados em 1980?"**

- Parte semÃ¢ntica: *filmes de terror*  
- Parte estrutural: *ano de lanÃ§amento = 1980*

Esse tipo de consulta pode ser tratado com uma estratÃ©gia chamada **LLM-Aided Retrieval**, onde o LLM ajuda a **entender, expandir ou reformular a consulta**, e a engine de busca aplica filtros estruturados.

#### ğŸ§  CompressÃ£o com LLM

ApÃ³s recuperar diversos chunks, Ã© possÃ­vel usar um LLM para **resumir, combinar ou comprimir** os resultados antes de adicionÃ¡-los ao prompt final.

Essa estratÃ©gia Ã© Ãºtil para:
- **Reduzir o custo e o tamanho do prompt**
- **Aumentar a densidade de informaÃ§Ã£o por token**

> âš ï¸ Naturalmente, hÃ¡ um **trade-off**: vocÃª pode ganhar espaÃ§o e velocidade, mas corre o risco de perder nuances importantes se a compressÃ£o for excessiva ou mal conduzida.
