# üß™ Laborat√≥rio LangChain com seus pr√≥prios dados

Este reposit√≥rio re√∫ne anota√ß√µes e c√≥digos experimentais baseados no curso ["LangChain: Chat with Your Data"](https://learn.deeplearning.ai/courses/langchain-chat-with-your-data/lesson/snupv/introduction), com foco em compreender e testar as etapas principais de processamento de documentos: **LOAD ‚Üí SPLIT ‚Üí EMBED ‚Üí RETRIEVE**.


## Baseado no curso em:

https://learn.deeplearning.ai/courses/langchain-chat-with-your-data/lesson/snupv/introduction


## üìÑ Documento usado nos testes

O PDF referenciado em alguns exemplos de c√≥digo vem do curso de Stanford. Para baix√°-lo, execute:

```bash
curl -O https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf
```

Salve em: **./docs/cs229_lectures/**

## Recomend√°vel criar o ambiente com virtualenv

```
virtualenv env
source env/bin/activate
```

## Depend√™ncias

```
pip install langchain pypdf yt_dlp pydub dotenv openai langchain-community beautifulsoup4 tiktoken langchain-openai chromadb
```

## O processo com o LangChain ocorre nas etapas LOAD, SPLIT, EMBED, RETRIEVE

### 1 - LOAD

Uma parte importante do LangChain s√£o seus DocumentLoaders, que podem carregar:

**Dados p√∫blicos**
- dados n√£o estruturados: youtube, wikipedia, twiter etc.
- dados estruturados: datasets, apis, csv etc.

**Dados privados**
- dados n√£o estruturados: power-point, notion, whatsapp etc.
- dados estruturados: pandas, excel, stripe, elastic etc.


#### 2 - SPLIT
Ap√≥s o carregamento, os documentos precisam ser divididos em peda√ßos menores (chunks) para que possam ser processados e indexados eficientemente ‚Äî especialmente em sistemas de busca vetorial.

##### Desafios do split
Dividir um texto apenas por n√∫mero de caracteres pode cortar frases ou ideias importantes no meio, dificultando a recupera√ß√£o de informa√ß√µes relevantes.

##### Estrat√©gias para preservar contexto
Uma pr√°tica comum √© usar overlapping entre chunks:

Por exemplo, se o chunk A vai de 0 a 500 tokens, o chunk B pode come√ßar no token 400 e ir at√© o 900, criando uma sobreposi√ß√£o de 100 tokens. Isso ajuda a manter o contexto fluido entre trechos consecutivos.


##### Principais Tipos de Text Splitters no LangChain

Os Text Splitters s√£o respons√°veis por dividir documentos em peda√ßos menores (**chunks**) para facilitar o processamento, indexa√ß√£o e busca. A escolha do splitter certo pode impactar diretamente a qualidade das respostas em sistemas baseados em RAG (retrieval-augmented generation).

###### CharacterTextSplitter

Divide o texto com base em um n√∫mero fixo de caracteres, sem considerar a estrutura sem√¢ntica. √â simples e r√°pido, mas pode cortar ideias no meio se n√£o for bem parametrizado.

```python
from langchain.text_splitter import CharacterTextSplitter
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)
```

###### MarkdownHeaderTextSplitter
Divide documentos com base na hierarquia de cabe√ßalhos Markdown (#, ##, ### etc.), preservando a estrutura do conte√∫do. Ideal para documenta√ß√£o t√©cnica.

```
from langchain.text_splitter import MarkdownHeaderTextSplitter
```

###### TokenTextSplitter
Usa contagem de tokens em vez de caracteres. √â √∫til para evitar que o chunk ultrapasse o limite de tokens de modelos como GPT.

```
from langchain.text_splitter import TokenTextSplitter
```

###### SentenceTransformersTokenTextSplitter
Usa embeddings para identificar pontos de corte sem√¢ntico entre senten√ßas, mantendo a coes√£o do texto. Requer a instala√ß√£o do sentence-transformers.

```
from langchain.text_splitter import SentenceTransformersTokenTextSplitter
```

###### RecursiveCharacterTextSplitter
O splitter mais recomendado na maioria dos casos. Tenta dividir primeiro por se√ß√µes maiores (como par√°grafos, frases) e vai recursivamente at√© atingir o tamanho ideal. Preserva bem o contexto.

```
from langchain.text_splitter import RecursiveCharacterTextSplitter
```

###### NLTKTextSplitter
Baseado na biblioteca NLTK, divide textos em senten√ßas respeitando a pontua√ß√£o e a gram√°tica. Requer o nltk instalado e seus recursos baixados.

```
from langchain.text_splitter import NLTKTextSplitter
```

###### SpacyTextSplitter
Semelhante ao NLTKTextSplitter, mas usa o spaCy para an√°lise sint√°tica mais precisa. Pode reconhecer entidades, frases nominais, etc.

```
from langchain.text_splitter import SpacyTextSplitter
```

###### Language (Code Splitter)

Pensado para c√≥digo-fonte. Usa a linguagem de programa√ß√£o como base para dividir trechos de c√≥digo por classes, fun√ß√µes ou blocos. Muito √∫til em agentes que lidam com bases de c√≥digo.

```
from langchain.text_splitter import Language
```


##### Informa√ß√µes interessantes
- Os chunks armazenam metadados relacionados a sua fonte - um chunk separado a partir de um pdf pode ter o nome do arquivo e a p√°gina em que foi produzido; um markdown pode ter a estrutura de cabe√ßalhos na qual se originou; etc


### 3 - EMBED

Ap√≥s carregar o documento e dividi-lo em peda√ßos menores (chunks), √© necess√°rio transformar cada chunk em um vetor num√©rico que represente seu conte√∫do de forma sem√¢ntica. Esse processo √© chamado de embedding, e o vetor resultante √© conhecido como um embedding vector (ou simplesmente embedding).

Esses vetores geralmente s√£o armazenados em uma base vetorial (vector store), permitindo buscas por similaridade com base no conte√∫do, em vez de apenas palavras-chave.

Neste exemplo usamos o base vetorial Chroma, mas poderia ser o FAISS, Weaviate etc.

Importante notar que neste caso estamos usando o OpenAIEmbeddings, consumindo a API da OpenAI. A vetoriza√ß√£o dos chunks **gera custo financeiro**. Em rela√ß√£o a isso, algumas dicas:

### üìä Pre√ßos (julho/2025)

- `text-embedding-3-small`: **0.02 USD por 1 milh√£o de tokens**
- `text-embedding-3-large`: **0.13 USD por 1 milh√£o de tokens**

Fonte oficial: [https://openai.com/pricing](https://openai.com/pricing)

### üî¢ Exemplo pr√°tico

Se voc√™ tiver 100 documentos com 500 tokens cada:

- Total de tokens: 100 √ó 500 = **50.000 tokens**
- Custo estimado com `text-embedding-3-small`:  
  ‚Üí 50.000 √ó 0.00002 USD = **0.01 USD**

### üí° Dicas para economizar

- Prefira o modelo `text-embedding-3-small`, que √© mais barato e eficiente.
- Ajuste o tamanho dos chunks para otimizar o n√∫mero de tokens por embedding.
- Use modelos **locais e gratuitos** caso a m√°xima precis√£o da OpenAI n√£o seja necess√°ria.

### üÜì Alternativas gratuitas (locais)

Voc√™ pode usar modelos open-source com `HuggingFaceEmbeddings` ou `LangChain`:

- `all-MiniLM-L6-v2`
- `intfloat/e5-small-v2`
- `thenlper/gte-small`

Esses modelos funcionam bem em pipelines RAG, sem custo por token.
