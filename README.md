# üß™ Laborat√≥rio LangChain com seus pr√≥prios dados

Este reposit√≥rio re√∫ne anota√ß√µes e c√≥digos experimentais baseados no curso ["LangChain: Chat with Your Data"](https://learn.deeplearning.ai/courses/langchain-chat-with-your-data/lesson/snupv/introduction), com foco em compreender e testar as etapas principais de processamento de documentos: **LOAD ‚Üí SPLIT ‚Üí EMBED ‚Üí RETRIEVE**.


## Baseado no curso em:

https://learn.deeplearning.ai/courses/langchain-chat-with-your-data/lesson/snupv/introduction


## üìÑ Documento usado nos testes

O PDF referenciado em alguns exemplos de c√≥digo vem do curso de Stanford. Para baix√°-lo, execute:

```bash
curl -O https://see.stanford.edu/materials/aimlcs229/transcripts/MachineLearning-Lecture01.pdf
```

Salve em: **./docs/cs229_lectures/**

## Recomend√°vel criar o ambiente com virtualenv

```
virtualenv env
source env/bin/activate
```

## Depend√™ncias

```
pip install langchain pypdf yt_dlp pydub dotenv openai langchain-community beautifulsoup4 tiktoken langchain-openai langchain_chroma chromadb lark scikit-learn jq
```

## O processo com o LangChain ocorre nas etapas LOAD, SPLIT, EMBED, RETRIEVE

### 1 - LOAD

Uma parte importante do LangChain s√£o seus DocumentLoaders, que podem carregar:

**Dados p√∫blicos**
- dados n√£o estruturados: youtube, wikipedia, twiter etc.
- dados estruturados: datasets, apis, csv etc.

**Dados privados**
- dados n√£o estruturados: power-point, notion, whatsapp etc.
- dados estruturados: pandas, excel, stripe, elastic etc.


#### 2 - SPLIT
Ap√≥s o carregamento, os documentos precisam ser divididos em peda√ßos menores (chunks) para que possam ser processados e indexados eficientemente ‚Äî especialmente em sistemas de busca vetorial.

##### Desafios do split
Dividir um texto apenas por n√∫mero de caracteres pode cortar frases ou ideias importantes no meio, dificultando a recupera√ß√£o de informa√ß√µes relevantes.

##### Estrat√©gias para preservar contexto
Uma pr√°tica comum √© usar overlapping entre chunks:

Por exemplo, se o chunk A vai de 0 a 500 tokens, o chunk B pode come√ßar no token 400 e ir at√© o 900, criando uma sobreposi√ß√£o de 100 tokens. Isso ajuda a manter o contexto fluido entre trechos consecutivos.


##### Principais Tipos de Text Splitters no LangChain

Os Text Splitters s√£o respons√°veis por dividir documentos em peda√ßos menores (**chunks**) para facilitar o processamento, indexa√ß√£o e busca. A escolha do splitter certo pode impactar diretamente a qualidade das respostas em sistemas baseados em RAG (retrieval-augmented generation).

###### CharacterTextSplitter

Divide o texto com base em um n√∫mero fixo de caracteres, sem considerar a estrutura sem√¢ntica. √â simples e r√°pido, mas pode cortar ideias no meio se n√£o for bem parametrizado.

```python
from langchain.text_splitter import CharacterTextSplitter
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)
```

###### MarkdownHeaderTextSplitter
Divide documentos com base na hierarquia de cabe√ßalhos Markdown (#, ##, ### etc.), preservando a estrutura do conte√∫do. Ideal para documenta√ß√£o t√©cnica.

```
from langchain.text_splitter import MarkdownHeaderTextSplitter
```

###### TokenTextSplitter
Usa contagem de tokens em vez de caracteres. √â √∫til para evitar que o chunk ultrapasse o limite de tokens de modelos como GPT.

```
from langchain.text_splitter import TokenTextSplitter
```

###### SentenceTransformersTokenTextSplitter
Usa embeddings para identificar pontos de corte sem√¢ntico entre senten√ßas, mantendo a coes√£o do texto. Requer a instala√ß√£o do sentence-transformers.

```
from langchain.text_splitter import SentenceTransformersTokenTextSplitter
```

###### RecursiveCharacterTextSplitter
O splitter mais recomendado na maioria dos casos. Tenta dividir primeiro por se√ß√µes maiores (como par√°grafos, frases) e vai recursivamente at√© atingir o tamanho ideal. Preserva bem o contexto.

```
from langchain.text_splitter import RecursiveCharacterTextSplitter
```

###### NLTKTextSplitter
Baseado na biblioteca NLTK, divide textos em senten√ßas respeitando a pontua√ß√£o e a gram√°tica. Requer o nltk instalado e seus recursos baixados.

```
from langchain.text_splitter import NLTKTextSplitter
```

###### SpacyTextSplitter
Semelhante ao NLTKTextSplitter, mas usa o spaCy para an√°lise sint√°tica mais precisa. Pode reconhecer entidades, frases nominais, etc.

```
from langchain.text_splitter import SpacyTextSplitter
```

###### Language (Code Splitter)

Pensado para c√≥digo-fonte. Usa a linguagem de programa√ß√£o como base para dividir trechos de c√≥digo por classes, fun√ß√µes ou blocos. Muito √∫til em agentes que lidam com bases de c√≥digo.

```
from langchain.text_splitter import Language
```


##### Informa√ß√µes interessantes
- Os chunks armazenam metadados relacionados a sua fonte - um chunk separado a partir de um pdf pode ter o nome do arquivo e a p√°gina em que foi produzido; um markdown pode ter a estrutura de cabe√ßalhos na qual se originou; etc


### 3 - EMBED

Ap√≥s carregar o documento e dividi-lo em peda√ßos menores (chunks), √© necess√°rio transformar cada chunk em um vetor num√©rico que represente seu conte√∫do de forma sem√¢ntica. Esse processo √© chamado de embedding, e o vetor resultante √© conhecido como um embedding vector (ou simplesmente embedding).

Esses vetores geralmente s√£o armazenados em uma base vetorial (vector store), permitindo buscas por similaridade com base no conte√∫do, em vez de apenas palavras-chave.

Neste exemplo usamos o base vetorial Chroma, mas poderia ser o FAISS, Weaviate etc.

Importante notar que neste caso estamos usando o OpenAIEmbeddings, consumindo a API da OpenAI. A vetoriza√ß√£o dos chunks **gera custo financeiro**. Em rela√ß√£o a isso, algumas dicas:

### üìä Pre√ßos (julho/2025)

- `text-embedding-3-small`: **0.02 USD por 1 milh√£o de tokens**
- `text-embedding-3-large`: **0.13 USD por 1 milh√£o de tokens**

Fonte oficial: [https://openai.com/pricing](https://openai.com/pricing)

### üî¢ Exemplo pr√°tico

Se voc√™ tiver 100 documentos com 500 tokens cada:

- Total de tokens: 100 √ó 500 = **50.000 tokens**
- Custo estimado com `text-embedding-3-small`:  
  ‚Üí 50.000 √ó 0.00002 USD = **0.01 USD**

### üí° Dicas para economizar

- Prefira o modelo `text-embedding-3-small`, que √© mais barato e eficiente.
- Ajuste o tamanho dos chunks para otimizar o n√∫mero de tokens por embedding.
- Use modelos **locais e gratuitos** caso a m√°xima precis√£o da OpenAI n√£o seja necess√°ria.

### üÜì Alternativas gratuitas (locais)

Voc√™ pode usar modelos open-source com `HuggingFaceEmbeddings` ou `LangChain`:

- `all-MiniLM-L6-v2`
- `intfloat/e5-small-v2`
- `thenlper/gte-small`

Esses modelos funcionam bem em pipelines RAG, sem custo por token.


### 4 - RETRIEVAL

Nesta etapa, consultamos de fato a base sem√¢ntica (vector store). A busca retorna, por padr√£o, os **chunks mais similares** √† consulta ‚Äî ou seja, aqueles cujo *embedding* tem menor dist√¢ncia vetorial em rela√ß√£o ao *embedding* da pergunta.

No entanto, **os resultados mais similares nem sempre s√£o os mais √∫teis**. Em muitos casos, pode ser mais interessante obter respostas que tragam **diversidade informativa**, e n√£o apenas repeti√ß√µes do mesmo contexto. √â aqui que entra o algoritmo **MMR (Maximal Marginal Relevance)**.

O MMR busca um **equil√≠brio entre relev√¢ncia e novidade**: resultados que sejam suficientemente **relacionados √† consulta**, mas ao mesmo tempo **diferentes entre si**, evitando redund√¢ncia.

#### üìå Exemplo ilustrativo

Imagine que um cozinheiro faz a pergunta:  
**"Quais cogumelos s√£o totalmente brancos?"**

- Os resultados mais similares podem descrever **uma √∫nica esp√©cie em detalhes** (como o champignon).
- Por√©m, um trecho que mencione que **"uma esp√©cie branca √© venenosa"**, mesmo sendo menos similar, pode ser **crucial** ‚Äî e o MMR ajuda a traz√™-lo para os resultados.

‚û°Ô∏è Em resumo: **MMR busca relev√¢ncia com a consulta, mas diversidade em rela√ß√£o aos demais resultados**.

---

### üîé Estrat√©gias adicionais de recupera√ß√£o

Al√©m da busca por similaridade ou MMR, existem outras estrat√©gias que podem ser aplicadas em sistemas baseados em embeddings:

#### üìò LLM-Aided Retrieval

Algumas consultas possuem tanto um **elemento sem√¢ntico** quanto um **filtro expl√≠cito**.  
Exemplo:  
**"Quais filmes de terror foram lan√ßados em 1980?"**

- Parte sem√¢ntica: *filmes de terror*  
- Parte estrutural: *ano de lan√ßamento = 1980*

Esse tipo de consulta pode ser tratado com uma estrat√©gia chamada **LLM-Aided Retrieval**, onde o LLM ajuda a **entender, expandir ou reformular a consulta**, e a engine de busca aplica filtros estruturados.

#### üß† Compress√£o com LLM

Ap√≥s recuperar diversos chunks, √© poss√≠vel usar um LLM para **resumir, combinar ou comprimir** os resultados antes de adicion√°-los ao prompt final.

Essa estrat√©gia √© √∫til para:
- **Reduzir o custo e o tamanho do prompt**
- **Aumentar a densidade de informa√ß√£o por token**

> ‚ö†Ô∏è Naturalmente, h√° um **trade-off**: voc√™ pode ganhar espa√ßo e velocidade, mas corre o risco de perder nuances importantes se a compress√£o for excessiva ou mal conduzida.

## 5 - QUESTION - ANSWER

O fluxo b√°sico √©:

1 - quest√£o √© submetida para o vector store
2 - vector store prov√™ n documentos relevantes
3 - quest√£o original e chunks s√£o enviados para LLM

### üß† Modos de resposta no LangChain: `stuff`, `map_reduce`, `refine` e `map_rerank`

Ao usar LangChain com m√∫ltiplos documentos (ou chunks), podemos escolher diferentes estrat√©gias para combinar as informa√ß√µes e gerar uma resposta final com o LLM.

---

#### 1. `stuff` ‚Äî Tudo de uma vez

> Envia todos os documentos concatenados diretamente no prompt do modelo.

- ‚úÖ Simples e r√°pido
- ‚ùå Limitado pelo tamanho m√°ximo de tokens do modelo
- Ideal para poucos documentos pequenos

```text
[documento1] + [documento2] + ... ‚Üí LLM responde com base em todos
```

#### 2. map_reduce ‚Äî Processamento paralelo + resumo

> O modelo processa cada chunk individualmente (map) e depois resume todas as respostas (reduce).

- üó∫ Map: o LLM responde cada documento separadamente
- üßæ Reduce: combina as respostas em um √∫nico resumo final
- ‚úÖ Escal√°vel para muitos documentos
- ‚ùå Pode perder o contexto global

```
[doc1 ‚Üí resp1], [doc2 ‚Üí resp2], ... ‚Üí resumo final das respostas
```

#### 3. refine ‚Äî Constru√ß√£o progressiva

> Cria uma resposta inicial com o primeiro chunk e refina iterativamente com os demais.

- Cada nova itera√ß√£o melhora ou expande a resposta anterior
- ‚úÖ Mant√©m coer√™ncia entre passos
- ‚ùå Pode propagar erros do in√≠cio se a resposta inicial for fraca

```
resp1 = resposta(doc1)  
resp2 = refinar(resp1 + doc2)  
resp3 = refinar(resp2 + doc3)  
... ‚Üí resposta final
```

#### 4. map_rerank ‚Äî Sele√ß√£o da melhor resposta

> O modelo avalia cada chunk individualmente e atribui uma pontua√ß√£o de relev√¢ncia a cada resposta.

- ‚úÖ Retorna a melhor resposta individual
- ‚ùå Pode ignorar outras informa√ß√µes relevantes

```
[doc1 ‚Üí (resposta1, score1)], [doc2 ‚Üí (resposta2, score2)], ...  
‚Üí retorna a resposta com maior score
```

#### üß≠ Qual usar?

| Modo           | Quando usar                                          |
-----------------|------------------------------------------------------|
| stuff          | Poucos documentos curtos                             |
| map_reduce     | Muitos documentos, resposta resumida                 |
| refine         | Deseja construir a resposta progressivamente         |
| map_rerank     | Quer selecionar a melhor resposta entre as poss√≠veis |

